"""
Network blocks for REINS.
"""

import torch
import torch.nn as nn


class MLPBnDrop(nn.Module):
    """
    MLP with BatchNorm and Dropout.

    Args:
        insize: Input dimension.
        outsize: Output dimension.
        hsizes: List of hidden layer sizes.
        nonlin: Activation function class (default: nn.ReLU).
        dropout: Dropout probability (default: 0.2, 0 to disable).
        bnorm: Enable BatchNorm (default: True).
        bias: Use bias in linear layers (default: True).
    """

    def __init__(self, insize, outsize, hsizes,
                 nonlin=nn.ReLU, dropout=0.2, bnorm=True, bias=True):
        super().__init__()
        sizes = [insize] + hsizes + [outsize]
        layers = []
        for i in range(len(sizes) - 2):
            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))
            layers.append(nonlin())
            if bnorm:
                layers.append(nn.BatchNorm1d(sizes[i + 1]))
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
        # Last layer: Linear only
        layers.append(nn.Linear(sizes[-2], sizes[-1], bias=bias))
        self.net = nn.Sequential(*layers)
        self.out_features = outsize

    def forward(self, *inputs):
        x = torch.cat(inputs, dim=-1) if len(inputs) > 1 else inputs[0]
        return self.net(x)
